MCP: Model context protocol
阿里云百炼
github: awesome-mcp-server
mcp.so
Smithery.ai
MCP实战指南，一口气搞定mcp与A2A竞争力翻倍（百度阿里全面拥抱mcp）
CherryStudio搭建本地AI知识库，三大痛点与进阶方案。
https://cloud.siliconflow.cn/
langchain

目前大模型知识库最常用的方法是RAG，当用户把资料添加进知识库的时候，程序会把他们拆分成很多个文本块，然后使用嵌入模型对这些文本块进行向量化，向量化指的是把切分后的文本变成一个超长的数字序列，
以硅基流动的嵌入模型BAAI/bge-m3为例，1024维，也就是每个文本块都被转换成一个1024数值的向量，然后程序把向量以及对应的文本保存在向量数据库里面，接下来，用户开始提问，这个提问不会直接送达到大模型
那里，而是把这个问题本身也经过向量化处理，先变成一个1024维的向量，然后把用户的提问与向量数据库进行形似度匹配，这个匹配过程是基于向量的纯数学运算，最后知识库选出匹配度最高的几个原文片段再加上
用户的问题发给大模型，大模型进行最后的归纳总结，从这个RAG流程可以看出大模型仅仅起归纳总结的作用，回答效果的好坏，很大程度上取决于文本块的检索精度。事实上RAG系统存在切片很粗暴、检索不精准、没有大局观
 RAG（Retrieval-Augmented Generation） ：检索增强生成，
弱点
1、切片粗暴：Cherry Studio使用的分块算法是Langchain的递归文本分割器，这种分块方法比较简单粗暴，基本上就等同于按文章的段落进行分块，如果段落过长，则按照固定的字数进行分块，因此很多句子可能会被拦腰截断，
2、检索不精准：搜索数据库的时候，是把用户的提问与向量数据库进行匹配，是基于纯数字的相似性计算，并不能代表文字本身的实际含义，所以筛选出来的文字片段可能跟用户的提问有关，也可能没有关系，没有办法做到完全的精准匹配
改进：使用重排序模型，把向量化初步检索出来的数据，使用专用的重排序模型进行更深入的语义分析，然后再按照问题的相关性进行重新排序，把相关性最大的一些数据排到最前面，并且交付给大模型，这是一种先粗后细的两步检索策略
3、没有大局观：无法处理结构化数据

